![任务1：线性回归](https://i.imgur.com/jgf4FKJ.jpg)
 
**1.线性回归损失函数的极大似然推导：西瓜书公式3.4除了用最小二乘法以外，怎么用极大似然推得？** 

![书中公式3.4](https://i.imgur.com/wZ4TcUf.jpg)

![第1题](https://i.imgur.com/CcUBvQl.jpg)    

**2.一元线性回归的参数求解公式推导：西瓜书公式3.7和3.8怎么推来的？**      
![第2题](https://i.imgur.com/5RRfCSL.jpg)
  
**3.多元线性回归的参数求解公式推导：西瓜书公式3.10和3.11怎么推来的？**   
![第3题](https://i.imgur.com/rKjQM5l.jpg) 

**4.线性回归损失函数的最优化算法：什么是批量梯度下降、随机梯度下降、小批量梯度下降。**  
批量梯度下降（Batch gradient descent，BGD）：遍历全部数据集计算一次loss函数，然后算函数对各个参数的梯度，更新梯度。 这种方法每更新一次参数都要把数据集里的所有样本都看一遍，计算量开销大，计算速度慢，不支持在线学习。BGD迭代的次数相对较少。

随机梯度下降（stochastic gradient descent，SGD）：每遍历一个数据就算一下loss函数，然后求梯度更新参数，这个方法速度比较快， 但是收敛性能不太好，可能在最优点附近晃来晃去，hit不到最优点。两次参数的更新也有可能互相抵消掉，造成目标函数震荡的比较剧烈。

小批量梯度下降（mini-batch gradient decent，MBGD）：把数据分为若干个批，按批来更新参数，这样，一个批中的一组数据共同决定了本次梯度的方向，下降起来就不容易跑偏，减少了随机性。另一方面因为批的样本数与整个数据集相比小了很多，计算量也不是很大。这是一种折衷的做法。

三种方法使用的情况：  
如果样本量比较小，采用批量梯度下降算法。如果样本太大，或者在线算法，使用随机梯度下降算法。在实际的一般情况下，采用小批量梯度下降算法。


参考资料：  
[批量梯度下降(BGD)、随机梯度下降(SGD)、小批量梯度下降(MBGD)](https://blog.csdn.net/zgcr654321/article/details/83027903)  

[常见优化算法批量梯度下降、小批量梯度下降、随机梯度下降的对比](https://blog.csdn.net/qq_36330643/article/details/79549520)